<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-05-30T18:26:31+08:00</updated><id>http://localhost:4000/</id><title type="html">Metapunk</title><subtitle>a place where I write down my notes and thoughts.</subtitle><author><name>Takchatsau</name></author><entry><title type="html">Some Supervised learning models</title><link href="http://localhost:4000/notes/Supervised-learning.html" rel="alternate" type="text/html" title="Some Supervised learning models" /><published>2018-05-27T00:00:00+08:00</published><updated>2018-05-27T00:00:00+08:00</updated><id>http://localhost:4000/notes/Supervised-learning</id><content type="html" xml:base="http://localhost:4000/notes/Supervised-learning.html">&lt;h2 id=&quot;监督学习supervised-learning的基本任务&quot;&gt;监督学习(Supervised learning)的基本任务&lt;/h2&gt;

&lt;p&gt;监督学习的基本任务就是，对于给定的training set，需要训练出一个function h(被定义为hypothesis)，使得h(x)对y有较好的预测。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ws1.sinaimg.cn/large/86223c22ly1frq7dwbd8nj20c208dmxx.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当y是连续的，这个学习任务被称为&lt;strong&gt;回归问题&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;当y是离散且有限的，这个学习任务被称为&lt;strong&gt;分类问题&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;线性回归&quot;&gt;线性回归&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/500px-Linear_regression.svg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;线性回归中hypothesis为一个线性函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{\theta}(x)=\theta_{0}+\sum^{n}_{i=1}\theta_{i}x_{i}&lt;/script&gt;

&lt;p&gt;其中$\theta$是$h_{\theta}(x)$的参数，实现从x到y的线性映射，令$x_{0}=1$,则，$h_{\theta}(x)$变为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{\theta}(x)=\sum^{n}_{i=0}\theta_{i}x_{i}=\theta^{T}x&lt;/script&gt;

&lt;p&gt;为了衡量y和$h_{\theta}(x)$之间的误差，定义&lt;strong&gt;cost function&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta)=\frac{1}{2}\sum^{m}_{i=1}(h_{\theta}(x^{(i)})-y^{(i)})^{2}&lt;/script&gt;

&lt;h4 id=&quot;lms-algorithm&quot;&gt;LMS algorithm&lt;/h4&gt;

&lt;p&gt;利用梯度下降法更新参数来minimize$J(\theta)$。&lt;/p&gt;

&lt;p&gt;对单个参数$\theta_{j}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\theta_{j}:=\theta_{j}-\alpha\frac{\partial J(\theta)}{\partial \theta_{j}}
\\
\begin{align*}
\frac{\partial J(\theta)}{\partial \theta_{j}} &amp;=\frac{1}{2}\frac{\partial }{\partial \theta_{j}}(h_{\theta}(x)-y)^{2}\\
&amp;=(h_{\theta}(x)-y)\frac{\partial }{\partial \theta_{j}}(h_{\theta}(x)-y)\\
&amp;=(h_{\theta}(x)-y)\frac{\partial }{\partial \theta_{j}}(\sum^{n}_{i=0}\theta_{i}x_{i}-y)\\
&amp;=(h_{\theta}(x)-y)x_{j}\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;所以，对于单一样本：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{j}:=\theta_{j}+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_{j}&lt;/script&gt;

&lt;p&gt;对全体样本：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{j}:=\theta_{j}+\alpha\sum_{i=1}^{m}(y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_{j}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;批梯度下降(batch gradient descent)：每次参数更新遍历全体样本。&lt;/li&gt;
  &lt;li&gt;随机梯度下降(stochastic/incremental gradient descent)：每次参数更新只随机遍历一个样本。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/1600/1*PV-fcUsNlD9EgTIc61h-Ig.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;迹运算的性质&quot;&gt;迹运算的性质&lt;/h4&gt;

&lt;p&gt;定义:
&lt;script type=&quot;math/tex&quot;&gt;\sum^{m}_{i=1}A_{i,i}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;性质：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;trAB=trBA
\\
trABC=trCAB=trBCA
\\
\nabla_{A}trAB=B^{T}
\\
tr(a)=a
\\
\nabla_{A}trABA^{T}C=CAB+C^{T}AB^{T}&lt;/script&gt;

&lt;p&gt;证明$\nabla_{A}trAB=B^{T}$ :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\frac{\partial trAB}{\partial A_{i,j}}&amp;=\frac{\partial \sum^{m}_{k=1}(AB)_{k,k}}{\partial A_{i,j}}
\\
&amp;=\frac{\partial (AB)_{i,i}}{\partial A_{i,j}}
\\
&amp;=\frac{\partial \sum^{m}_{l=1}A_{i,l}B_{l,i}}{\partial A_{i,j}}
\\
&amp;=\frac{\partial A_{i,j}B_{j,i}}{\partial A_{i,j}}
\\
&amp;=B_{j,i}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;所以$\nabla_{A}trAB=B^{T}$。&lt;/p&gt;

&lt;p&gt;证明$\nabla_{A}trABA^{T}C=CAB+C^{T}AB^{T}$：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\nabla_{A}trABA^{T}C&amp;=\nabla_{A}tr\underbrace{AB}_{u(A)}\underbrace{A^{T}C}_{v(A^{T})}
\\
&amp;=\nabla_{A:u(A)}tr(u(A)v(A^{T}))+\nabla_{A:v(A^{T})}tr(u(A)v(A^{T}))
\\
&amp;=(v(A^{T}))^{T}\nabla_{A}u(A)+(\nabla_{A^{T}:v(A^{T})}tr(u(A)v(A^{T})))^{T}
\\
&amp;=C^{T}AB^{T}+((u(A))^{T}\nabla_{A^{T}}v(A^{T}))^{T}
\\
&amp;=C^{T}AB^{T}+(B^{T}A^{T}C^{T})^{T}
\\
&amp;=C^{T}AB^{T}+CAB
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;利用迹运算的性质推导jθ的梯度&quot;&gt;利用迹运算的性质推导J(θ)的梯度&lt;/h4&gt;

&lt;p&gt;将training set写成如下形式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X=\begin{bmatrix} (x^{(1)})^{T} \\ \vdots \\  (x^{(m)})^{T}
\end{bmatrix}&lt;/script&gt;

&lt;p&gt;m为training set数目。&lt;/p&gt;

&lt;p&gt;则有：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X\theta=\begin{bmatrix}
(x^{(1)})^{T}\theta \\ \vdots \\  (x^{(m)})^{T}\theta
\end{bmatrix}
=
\begin{bmatrix}
h_{\theta}(x^{(1)}) \\ \vdots \\  h_{\theta}(x^{(m)})
\end{bmatrix}
\\
J(\theta)=\frac{1}{2}(X\theta-y)(X\theta-y)^{T}
\\
\nabla_{\theta}J(\theta)=\frac{1}{2}\nabla_{\theta}(\theta^{T}X^{T}X\theta-\theta^{T}X^{T}y-y^{T}X\theta+y^{T}y)&lt;/script&gt;

&lt;p&gt;由于J(θ)为一个实数，则有：&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
\nabla_{\theta}J(\theta)&amp;=\frac{1}{2}\nabla_{\theta}tr(\theta^{T}X^{T}X\theta-\theta^{T}X^{T}y-y^{T}X\theta+y^{T}y)
\\
&amp;=\frac{1}{2}\nabla_{\theta}tr(\theta\theta^{T}X^{T}X)-\nabla _{\theta}tr(y^{T}X\theta)
\\
\end{align*} %]]&gt;&lt;/script&gt;
其中：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\nabla_{\theta}tr(\theta\theta^{T}X^{T}X)&amp;=\nabla_{\theta}tr(\underbrace{\theta}_{A}\underbrace{I}_{B}\underbrace{\theta^{T}}_{A^{T}}\underbrace{X^{T}X}_{C})
\\
&amp;=X^{T}X\theta I+X^{T}X\theta I
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;所以：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\nabla_{\theta}J(\theta)
&amp;=\frac{1}{2}\nabla_{\theta}tr(\theta\theta^{T}X^{T}X)-\nabla _{\theta}tr(y^{T}X\theta)
\\
&amp;=X^{T}X\theta-X^{T}y
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;要使J(θ)达到最小，令$\nabla_{\theta}J(\theta)=0$，即得到normal equation：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;X^{T}X\theta=X^{T}y&lt;/script&gt;

&lt;p&gt;求解得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta=(X^{T}X)^{-1}X^{T}y&lt;/script&gt;

&lt;h4 id=&quot;lwrlocally-weighted-linear-regression&quot;&gt;LWR(Locally weighted linear regression)&lt;/h4&gt;

&lt;p&gt;cost function变成了：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta)=\sum_{i}w^{(i)}(y^{(i)}-\theta^{T}x^{(i)})^{2}
\\
w^{(i)}=exp(-\frac{(x^{(i)}-x)^{2}}{2\tau^{2}})&lt;/script&gt;

&lt;p&gt;$w^{(i)}$被称为权重(weights)，$\tau$被称为&lt;strong&gt;波长函数(bandwidth)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;由于对于每一个x，都要重新计算一次weights，所以每求一个h(x)，θ也要再求一遍。&lt;/p&gt;

&lt;h5 id=&quot;parametric-learning-algorithm--non-parametric-learning&quot;&gt;Parametric learning algorithm &amp;amp; Non-parametric learning&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;parametric learning algorithm：用若干个固定、有限的参数来拟合训练集的数据，训练完成后，只保存这些参数用于测试过程，而不会再参考训练集的数据。&lt;/li&gt;
  &lt;li&gt;non-parametric learning algorithm：测试的过程需要利用训练集的数据进行计算，随着训练集数目的增长，hypothesis h的数目线性增长。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;用概率论解释jθ的意义&quot;&gt;用概率论解释J(θ)的意义&lt;/h4&gt;

&lt;p&gt;在目标变量y和h(x)之间引入误差项：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y^{(i)}=\theta^{T}x^{(i)}+\epsilon^{(i)}&lt;/script&gt;

&lt;p&gt;误差项符合均值为0的高斯分布：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon^{(i)}\thicksim \mathcal{N}(0,\sigma^{2})&lt;/script&gt;

&lt;p&gt;则y的条件概率可以表示为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y^{(i)}| x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma^{2}})&lt;/script&gt;

&lt;p&gt;由于对每个训练样本的误差项是独立同分布(IID)的，那么整个训练集的似然(likelihood)可以写成：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta)=\prod_{i=1}^{m}P(y^{(i)}| x^{(i)};\theta)&lt;/script&gt;

&lt;p&gt;取对数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
l(\theta)=ln(L(\theta))&amp;=\sum^{m}_{i=1}lnP(y^{(i)}| x^{(i)};\theta)
\\
&amp;=-mln(\sqrt{2\pi}\sigma)-\frac{1}{2\sigma^{2}}\sum^{m}_{i=1}(y^{(i)}-\theta^{T}x^{(i)})^{2}
\end{align*}
\\
so,maximize \ L(\theta) \ \iff \ minimize \ J(\theta)=\frac{1}{2}\sum^{m}_{i=1}(h_{\theta}(x^{(i)})-y^{(i)})^{2} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;逻辑回归logistic-regression&quot;&gt;逻辑回归(Logistic regression)&lt;/h3&gt;

&lt;p&gt;y只取0和1两个值，h(x)输出y=1的概率值：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{\theta}(x)=g(\theta^{T}x)=\frac{1}{1+e^{-\theta^{T}x}}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Sigmoid-function-2.svg/500px-Sigmoid-function-2.svg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;g(z)的导数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\frac{dg(z)}{dz}&amp;=\frac{e^{-z}}{(1+e^{-z})^{2}}
\\
&amp;=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}}) 
\\
&amp;=g(z)(1-g(z))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;假设：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y=1|x;\theta)=h_{\theta}(x)
\\
P(y=0|x;\theta)=1-h_{\theta}(x)&lt;/script&gt;

&lt;p&gt;那么：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y|x;\theta)=(h_{\theta}(x))^{y}(1-h_{\theta}(x))^{1-y}&lt;/script&gt;

&lt;p&gt;似然函数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
L(\theta)&amp;=\prod^{m}_{i=1}p(y^{(i)}|x^{(i)};\theta)
\\
&amp;=\prod^{m}_{i=1}(h_{\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}
\end{align*}
\\
l(\theta)=ln(L(\theta))=\sum^{m}_{i=1}y^{(i)}lnh(x^{(i)})+(1-y^{(i)})ln(1-h(x^{(i)})) %]]&gt;&lt;/script&gt;

&lt;p&gt;求导：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\frac{\partial l(\theta)}{\partial\theta_{j}}&amp;=(y\frac{1}{g(\theta^{T}x)}-(1-y)\frac{1}{1-g(\theta^{T}x)})\frac{\partial g(\theta^{T}x)}{\partial \theta_{j}} 
\\
&amp;=(y\frac{1}{g(\theta^{T}x)}-(1-y)\frac{1}{1-g(\theta^{T}x)})g(\theta^{T}x)(1-g(\theta^{T}x))\frac{\partial \theta^{T}x}{\partial \theta_{j}} 
\\
&amp;=(y(1-g(\theta^{T}x))-(1-y)g(\theta^{T}x))x_{j}
\\
&amp;=(y-h_{\theta}(x))x_{j}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;所以梯度下降法的update rule为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{j}:=\theta_{j}+\alpha\sum_{i=1}^{m}(y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_{j}&lt;/script&gt;

&lt;h4 id=&quot;感知机算法the-perceptron-learning-algorithm&quot;&gt;感知机算法(The perceptron learning algorithm)&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
h_{\theta}(x)=g(\theta^{T}x)
\\
where \quad g(z)=\left\{\begin{matrix}1\quad if\ z\ge0\\0\quad if\ z&lt;0\end{matrix}
\right. %]]&gt;&lt;/script&gt;

&lt;p&gt;其实，g(z)可由sigmoid函数取极限求得：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(z)=\lim_{\beta\rightarrow\infty}\sigma(\beta z)=\frac{1}{1+e^{-\beta z}}\\&lt;/script&gt;

&lt;p&gt;所以：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\quad\frac{\partial l(\theta)}{\partial\theta_{j}}\\
&amp;=\lim_{\beta\rightarrow \infty}(y\frac{1}{\sigma(\theta^{T}\beta x)}-(1-y)\frac{1}{1-\sigma(\theta^{T}\beta x)})\sigma(\theta^{T}\beta x)(1-\sigma(\theta^{T}\beta x))\frac{\partial \theta^{T}\beta x}{\partial \theta_{j}} 
\\
&amp;=\lim_{\beta\rightarrow\infty}(y(1-\sigma(\theta^{T}\beta x))-(1-y)\sigma(\theta^{T}\beta x))\beta x_{j}
\\
&amp;=\lim_{\beta\rightarrow\infty}(y-h_{\theta}(x))\beta x_{j}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;令β&amp;gt;0，则向量$v_{j}=(y-h_{\theta}(x))x_{j}​$的方向与l(θ)梯度$\nabla_{\theta}l(\theta)​$的方向相同，所以感知机的update rule仍然可以写成：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{j}:=\theta_{j}+\alpha\sum_{i=1}^{m}(y^{(i)}-h_{\theta}(x^{(i)}))x^{(i)}_{j}&lt;/script&gt;

&lt;h3 id=&quot;牛顿法求极值&quot;&gt;牛顿法求极值&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://ecourses.ou.edu/ebook/math/ch03/sec034/media/dia03422.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;目标：f(θ)  find  θ  s.t.  f(θ)=0.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta:=\theta-\frac{f(\theta)}{f^{'}(\theta)}&lt;/script&gt;

&lt;p&gt;对于l(θ)，其在最大值处的导数为0，所以update rule：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta:=\theta-\frac{l^{'}(\theta)}{l^{''}(\theta)}&lt;/script&gt;

&lt;p&gt;写成矩阵形式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta:=\theta-H^{-1}\nabla_{\theta}l(\theta)&lt;/script&gt;

&lt;p&gt;H为Hessian矩阵：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{i,j}=\frac{\partial^{2}l(\theta)}{\partial \theta_{i}\partial\theta_{j}}&lt;/script&gt;

&lt;p&gt;&lt;em&gt;&lt;u&gt;Newton method虽然收敛得很快，但面对数目较大的训练集，需要计算一个很大的Hessian 矩阵并求逆，反而降低了更新的速度。&lt;/u&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;广义线性模型generalized-linear-models&quot;&gt;广义线性模型(Generalized Linear Models)&lt;/h3&gt;

&lt;h4 id=&quot;指数分布族the-exponential-family&quot;&gt;指数分布族(The exponential family)&lt;/h4&gt;

&lt;p&gt;分布函数具有以下形式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y;\eta)=b(y)exp(\eta^{T}T(y)-a(\eta))&lt;/script&gt;

&lt;p&gt;其中，η被称为自然参数(natural parameter)，T(y)被称为充分统计量(sufficient statistic，一般T(y)=y)，a(η)为log partition function，使得分布函数的积分为1。&lt;/p&gt;

&lt;h5 id=&quot;bernoulli分布&quot;&gt;Bernoulli分布：&lt;/h5&gt;

&lt;p&gt;对Ber(Φ)：P(y=1;Φ)=Φ，P(y=0;Φ)=1-Φ，则：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
P(y;\phi)&amp;=\phi^{y}(1-\phi)^{(1-y)}
\\
&amp;=exp[yln\phi+(1-y)ln(1-\phi)]
\\
&amp;=exp[\underbrace{y}_{T(y)}\underbrace{ln\frac{\phi}{1-\phi}}_{\eta}+\underbrace{ln(1-\phi)}_{a(\eta)}]
\end{align*}
\\
\eta=ln\frac{\phi}{1-\phi}\ \Longrightarrow \ \phi=\frac{1}{1+e^{-\eta}}
\\
a(\eta)=-ln(1-\phi)=ln(1+e^{-\eta}) %]]&gt;&lt;/script&gt;

&lt;h5 id=&quot;gaussian分布&quot;&gt;Gaussian分布：&lt;/h5&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(y;\mu,\sigma^{2})&amp;=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y-\mu)^{2}}{2\sigma^{2}})
\\
&amp;=\underbrace{\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{y^{2}}{2\sigma^{2}})}_{b(y)}exp(\underbrace{\mu}_{\eta}\underbrace{\frac{y}{\sigma^{2}}}_{T(y)}-\underbrace{\frac{\mu^{2}}{2\sigma^{2}}}_{a(\eta)})
\end{align*}
\\
a(\eta)=\frac{\eta^{2}}{2\sigma^{2}} %]]&gt;&lt;/script&gt;

&lt;h4 id=&quot;glm&quot;&gt;GLM:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;假设$y\vert x;θ$ 满足指数族分布&lt;/li&gt;
  &lt;li&gt;对给定x，输出目标为$E[T(y)\vert x]$，所以拟合目标为$h(x)=E[T(y)\vert x]$&lt;/li&gt;
  &lt;li&gt;η是x的线性运算结果:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta=\theta^{T}x(\eta_{i}=\theta_{i}^{T}x , if \ \eta\in\mathbb{R}^{k})&lt;/script&gt;

&lt;p&gt;&lt;em&gt;对于二分类问题&lt;/em&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_{\theta}(x)=E[y|x;\theta]=P(y=1|x;\theta)=\phi=\frac{1}{1+e^{-\eta}}=\frac{1}{1+e^{-\theta^{T}x}}&lt;/script&gt;

&lt;h4 id=&quot;softmax-regression&quot;&gt;Softmax Regression&lt;/h4&gt;

&lt;p&gt;对于多分类问题，$y\in {1,\dots,k}$，则可以让GLM输出k-1个参数：$\phi_{1}\dots\phi_{k-1}$，另外$\phi_{k}=1-\sum^{k-1}&lt;em&gt;{i=1}\phi&lt;/em&gt;{i}$，同样令$P(y=i)=\phi_{i}$，y的分布为多项式分布，为了让其能表达成指数分布族的形式，将T(y)写成向量形式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;T(1)=\begin{bmatrix}1\\0\\0\\\vdots\\0\end{bmatrix},\dots,T(k-1)=\begin{bmatrix}0\\0\\0\\\vdots\\1\end{bmatrix},
\dots,T(k)=\begin{bmatrix}0\\0\\0\\\vdots\\0\end{bmatrix}\\
T(y)\in\mathbb{R}^{k-1}&lt;/script&gt;

&lt;p&gt;引入符号1{.}(1{True}=1,1{False}=0),则：&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;(T(y)){i}=1{y=i},E[(T(y)){i}]=P(y=i)=\phi_{i}&lt;/script&gt;
证明p(y;Φ)可写成指数分布族的形式：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(y;\phi)&amp;=\prod^{k}_{i=1}\phi_{i}^{1\{y=i\}}\\
&amp;=\phi_{k}^{1-\sum^{k-1}_{l=1}(T(y))_{l}}\prod^{k-1}_{i=1}\phi_{i}^{(T(y))_{i}}\\
&amp;=exp[\sum^{k-1}_{i=1}(T(y))_{i}ln(\phi_{i})+(1-\sum^{k-1}_{l=1}(T(y))_{l})ln(\phi_{k})]\\
&amp;=exp[\sum^{k-1}_{i=1}(T(y))_{i}ln(\frac{\phi_{i}}{\phi_{k}})+ln(\phi_{k})]\\
&amp;=b(y)exp[\eta^{T}T(y)-a(\eta)]
\end{align*}
\\
\quad \\
\quad \\
where\quad\eta_{i}=ln(\frac{\phi_{i}}{\phi_{k}}),a(\eta)=-ln(\phi_{k}),b(y=1) %]]&gt;&lt;/script&gt;

&lt;p&gt;然后：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
e^{\eta_{i}}&amp;=\frac{\phi_{i}}{\phi_{k}}\\
\phi_{k}e^{\eta_{i}}&amp;=\phi_{i}\\
\phi_{k}\sum^{k}_{i=1}e^{\eta_{i}}&amp;=\sum^{k}_{i=1}\phi_{i}=1\\
\phi_{k}&amp;=\frac{1}{\sum^{k}_{i=1}e^{\eta_{i}}}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;所以：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(y=i|x;\theta)&amp;=\phi_{i}\\
&amp;=\frac{e^{\eta_{i}}}{\sum^{k}_{j=1}e^{\eta_{j}}}\\
&amp;=\frac{e^{\theta^{T}_{i}x}}{\sum^{k}_{j=1}e^{\theta_{j}^{T}x}}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;h(x)表示y=i的概率，所以：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
h_{\theta}(x)&amp;=E[T(y)|x;\theta]\\
&amp;=E\begin{bmatrix}\begin{matrix}1\{y=1\}\\\vdots\\1\{y=k-1\}\end{matrix}\Bigg | x;\theta\end{bmatrix}\\
&amp;=\begin{bmatrix}\phi_{i}\\\vdots\\\phi_{k-1}\end{bmatrix}\\
&amp;=\begin{bmatrix}\frac{exp(\theta_{1}^{T}x)}{\sum^{k}_{j=1}exp(\theta^{T}_{j}x)}\\\vdots\\\frac{exp(\theta_{k-1}^{T}x)}{\sum^{k}_{j=1}exp(\theta^{T}_{j}x)}\end{bmatrix}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;而：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y=k|x;\theta)=\phi_{k}=1-\sum^{k-1}_{i=1}\phi_{i}&lt;/script&gt;

&lt;p&gt;似然函数l(θ)：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
l(\theta)&amp;=\sum^{m}_{i=1}logp(y^{(i)}|x^{(i)};\theta)\\
&amp;=\sum^{m}_{i=1}log\prod^{k}_{l=1}\begin{pmatrix}\frac{e^{\theta_{l}^{T}x^{(i)}}}{\sum^{k}_{i=1}e^{\theta_{j}^{T}x^{(i)}}}\end{pmatrix}^{1\{y^{(i)}=l\}}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;然后，可用梯度上升或者牛顿法来求得θ，使得似然函数l(θ)取得极大值。&lt;/p&gt;</content><author><name>Takchatsau</name></author><category term="notes" /><summary type="html">监督学习(Supervised learning)的基本任务</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/cybercity-1.jpg" /></entry><entry><title type="html">向量、矩阵运算的求导</title><link href="http://localhost:4000/notes/Tensor-bp.html" rel="alternate" type="text/html" title="向量、矩阵运算的求导" /><published>2018-05-02T00:00:00+08:00</published><updated>2018-05-02T00:00:00+08:00</updated><id>http://localhost:4000/notes/Tensor-bp</id><content type="html" xml:base="http://localhost:4000/notes/Tensor-bp.html">&lt;h2 id=&quot;jacobian矩阵&quot;&gt;Jacobian矩阵&lt;/h2&gt;

&lt;p&gt;对于映射$f:\mathbb{R}^{N}\rightarrow\mathbb{R}^{M}$，对各个output求导会获得一个$M\times N$的偏微分矩阵：
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\frac{\partial y}{\partial x}=\left(\begin{matrix}
\frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{N}} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_{M}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{M}}{\partial x_{N}}
\end{matrix}\right) %]]&gt;&lt;/script&gt;
于是显然可得，在线性分类器中，$\vec{y}=W\vec{x}$，x对y的偏导有简单结果：
&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial \vec{y}_{i}}{\partial \vec{x}_{j}}=W_{i,j}
\\
即
\\
\frac{\partial \vec{y}}{\partial \vec{x}}=W&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;对矩阵相乘:&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;Y=XW \\
 Y_{i,j}=\sum^{D}_{k=1}X_{i,k}{W_{k,j}} \\
 \frac{\partial Y_{i,j}}{\partial X_{i,k} }=W_{k,j}
 \\
 则，\frac{\partial Y_{i,:}}{\partial X_{i,:} }=W，其余情况均为0。&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;链式法则&quot;&gt;链式法则：&lt;/h3&gt;

&lt;p&gt;上述形式也能适用链式法则，对：&lt;/p&gt;

&lt;p&gt;$\vec{y}=VW\vec{x}$，显然，$\frac{d\vec{y}}{d\vec{x}}=VW$。&lt;/p&gt;

&lt;p&gt;定义中间变量：$\vec{m}=W\vec{x}$，则$\vec{y}=V\vec{m}$。&lt;/p&gt;

&lt;p&gt;依照链式法则：
&lt;script type=&quot;math/tex&quot;&gt;\frac{d\vec{y}}{d\vec{x}}=\frac{d\vec{y}}{d\vec{m}}\frac{d\vec{m}}{d\vec{x}}
\\
\frac{d\vec{y_{i}}}{d\vec{x_{j}}}=\frac{d\vec{y_{i}}}{d\vec{m}}\frac{d\vec{m}}{d\vec{x_{j}}}=\sum^{M}_{k=1}\frac{d\vec{y_{i}}}{d\vec{m_{k}}}\frac{d\vec{m_{k}}}{d\vec{x_{j}}}
\\
\frac{d\vec{y_{i}}}{d\vec{x_{j}}}=(VK)_{i,j}=\sum^{M}_{k=1}V_{i,k}W_{k,j}&lt;/script&gt;
即对于矩阵或向量的复合函数求导，亦可以依照链式法则，将各中间变量的雅可比矩阵相乘得到最后的偏导。&lt;/p&gt;</content><author><name>Metapunk</name></author><category term="notes" /><summary type="html">Jacobian矩阵</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/neon.jpg" /></entry><entry><title type="html">初涉反向传播</title><link href="http://localhost:4000/notes/Backpropagation.html" rel="alternate" type="text/html" title="初涉反向传播" /><published>2018-05-01T00:00:00+08:00</published><updated>2018-05-01T00:00:00+08:00</updated><id>http://localhost:4000/notes/Backpropagation</id><content type="html" xml:base="http://localhost:4000/notes/Backpropagation.html">&lt;h2 id=&quot;求谁的梯度&quot;&gt;求谁的梯度？&lt;/h2&gt;

&lt;p&gt;$W,b,x_{i}$都是作为$loss function$的变量，但是学习的过程中，要优化的是$W,b$ ，所以在反向传播算法中，我们计算的都是$W,b$的梯度。&lt;/p&gt;

&lt;h2 id=&quot;计算图&quot;&gt;计算图&lt;/h2&gt;

&lt;p&gt;一些简单的多元函数的偏导：
&lt;script type=&quot;math/tex&quot;&gt;f(x,y)=xy \longrightarrow\frac{\partial f}{\partial x}=y,\frac{\partial f}{\partial y}=x
\\
f(x,y)=x+y\longrightarrow\frac{\partial f}{\partial x}=\frac{\partial f}{\partial y}=1
\\
f(x,y)=max(x,y)\longrightarrow\frac{\partial f}{\partial x}=\mathbb{I}(x\ge y),\frac{\partial f}{\partial y}=\mathbb{I}(y\ge x)&lt;/script&gt;
$\mathbb{I}$是标识运算符，当括号内为真时，值为1，否则为0。&lt;/p&gt;

&lt;h4 id=&quot;对复合函数求导&quot;&gt;对复合函数求导&lt;/h4&gt;

&lt;p&gt;使用链式法则：
&lt;script type=&quot;math/tex&quot;&gt;f(x,y,z)=(x+y)z,q=x+y
\\
f=qz,则\frac{\partial f}{\partial x}=\frac{\partial f}{\partial q}\frac{\partial q}{\partial x}=z\cdot1&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dfdz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dfdq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dfdx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dfdq&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dfdy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dfdq&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;各个gate的偏导依照链式法则相乘求得最终各变量的偏导。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;backpropagation is a beautifully local process&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;图中每个gate在接受到输入数据后可以马上进行两件事：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;计算当前gate的output&lt;/li&gt;
  &lt;li&gt;计算各个变量对这个output的局部偏导&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;网路跑完前馈之后，反向传播过程便将数据流过路径的偏导相乘起来。&lt;/p&gt;

&lt;h4 id=&quot;sigmoid-gate&quot;&gt;sigmoid gate&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma(x)=\frac{1}{1+e^{-x}}
\\
\frac{\partial \sigma(x)}{\partial x}=(1-\sigma(x))\sigma(x)(过程略)&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsigdx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;有时候变量会在多个复合函数之中，不要忘记在最后将他们相加&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;如：
&lt;script type=&quot;math/tex&quot;&gt;f(x,y)=\frac{x+\sigma(y)}{\sigma(x)+(x+y)^{2}}&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;sigy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigy&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sigx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xpysqr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;den&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xpysqr&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;invdev&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;den&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;invden&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#backpropagation&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dnum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;invden&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dinvden&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dden&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;den&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dinvden&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsigx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dden&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dxpysqr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dden&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dxpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dxpysqr&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dxpy&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dxpy&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dsigx&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dnum&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsigy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dnum&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dsigy&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;一个隐性的问题&lt;/strong&gt;:如果一个很大的数与一个很小的数相乘时，根据乘法gate求导规则，很小的数对应的变量得到的梯度会很大，很大的数对应的变量得到的梯度会很小。比如在linear classifiers中，$w^{T}x_{i}$ 对于很大的输入，权重更新的梯度也会很大，则需要减小学习率来进行补偿。所以，需要在数据输入之前对其进行适当的预处理。&lt;/p&gt;</content><author><name>Metapunk</name></author><category term="notes" /><summary type="html">求谁的梯度？</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/thor.jpg" /></entry><entry><title type="html">没有银弹</title><link href="http://localhost:4000/thoughts/No-silver-bullet.html" rel="alternate" type="text/html" title="没有银弹" /><published>2018-04-30T00:00:00+08:00</published><updated>2018-04-30T00:00:00+08:00</updated><id>http://localhost:4000/thoughts/No-silver-bullet</id><content type="html" xml:base="http://localhost:4000/thoughts/No-silver-bullet.html">&lt;h1 id=&quot;no-silver-bullet&quot;&gt;No silver bullet.&lt;/h1&gt;</content><author><name>Metapunk</name></author><category term="thoughts" /><summary type="html">No silver bullet.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/cowboy-1.jpg" /></entry><entry><title type="html">深度学习预备知识(Part 1)</title><link href="http://localhost:4000/notes/Deeplearning.html" rel="alternate" type="text/html" title="深度学习预备知识(Part 1)" /><published>2018-04-29T00:00:00+08:00</published><updated>2018-04-29T00:00:00+08:00</updated><id>http://localhost:4000/notes/Deeplearning</id><content type="html" xml:base="http://localhost:4000/notes/Deeplearning.html">&lt;h2 id=&quot;线性代数&quot;&gt;线性代数&lt;/h2&gt;

&lt;h4 id=&quot;向量的范数norm&quot;&gt;向量的&lt;strong&gt;范数&lt;/strong&gt;(norm)&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;||x||_{p}=(\sum_{i}|x|^{p})^\frac{1}{p} \\
其中p\in \mathbb{R},p\ge 1&lt;/script&gt;

&lt;p&gt;$L^{1}$范数经常作为表示非零元素数目的替代函数&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最大范数$L^{\infty}$：&lt;/strong&gt;表示向量中具有最大幅值的元素的绝对值
&lt;script type=&quot;math/tex&quot;&gt;||x||_{\infty}=\max_{i}|x_{i}|&lt;/script&gt;
 在深度学习中，会衡量矩阵的大小，最常用&lt;strong&gt;Frobenius范数&lt;/strong&gt; (Frobenius norm)
&lt;script type=&quot;math/tex&quot;&gt;||A||_{F}=\sqrt{\sum_{i,j}A^{2}_{i,j}}&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;特殊类型的矩阵和向量&quot;&gt;特殊类型的矩阵和向量&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;对角矩阵&lt;/strong&gt;(diagonal matrix)只在主对角线上含有非零元素，其他位置都是零。用diag(v)表示。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;对称矩阵&lt;/strong&gt;(symmetric matrix)是转置和自己相等的矩阵。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;正交矩阵&lt;/strong&gt;(orthogonal matrix)指行向量和列向量是分别标准正交的方阵，即
&lt;script type=&quot;math/tex&quot;&gt;A^{\top}A=AA^{\top}=I \\
 A^{-1}=A^{\top}&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;特征分解&quot;&gt;特征分解&lt;/h3&gt;

&lt;p&gt;方阵$A$的&lt;strong&gt;特征向量&lt;/strong&gt; (eigenvector)是指与$A$相乘后相当于对该向量进行缩放的非零向量&lt;strong&gt;v&lt;/strong&gt;：
&lt;script type=&quot;math/tex&quot;&gt;\mathit{Av=\lambda v}&lt;/script&gt;
其中标量$\mathit{\lambda}$称为这个特征向量对应的&lt;strong&gt;特征值&lt;/strong&gt;(eigenvalue)。&lt;/p&gt;

&lt;p&gt;假设矩阵$A$ 有$n$ 个线性无关的特征向量${v^{(1)},\cdots,v^{(n)}}$，对应着特征值${\lambda_{1},\cdots,\lambda_{n}}$。 将特征向量连接成一个矩阵，使得每一列是一个特征向量：$V=[v^{(1)},\cdots,v^{(n)}]$ 。类似地，将特征值连接成一个向量$\lambda=[\lambda_{1},\cdots,\lambda_{n}]^{\top}$。因此$A$的&lt;strong&gt;特征分解&lt;/strong&gt;(eigendecomposition)可以记作：
&lt;script type=&quot;math/tex&quot;&gt;A=Vdiag(\lambda)V^{-1}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;每个实对称矩阵都可以分解成实特征向量和实特征值：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A=Q\Lambda Q^{\top}&lt;/script&gt;

&lt;p&gt;其中$Q$是$A$的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。特征值$\Lambda_{i,i}$对应的特征向量是矩阵$Q$的第$i$列，记作$Q_{:,i}$。&lt;/p&gt;

&lt;p&gt;如果两个或多个特征向量拥有相同的特征值，那么在由这些特征向量产生的生成子空间中，任意一组正交向量都是该特征值对应的特征向量。因此，我们可以等价地从这些特征向量中构成$Q$作为替代。按照惯例，我们通常按降序排列$\Lambda$的元素。在该约定下，特征分解唯一，当且仅当所有的特征值都是唯一的。&lt;/p&gt;

&lt;p&gt;所有特征值都是正数的矩阵称为&lt;strong&gt;正定&lt;/strong&gt;(positive definite)；所有特征值都是非负数的矩阵称为&lt;strong&gt;半正定&lt;/strong&gt;(positive semidefinite)。同样地，所有特征值都是负数的&lt;strong&gt;负定&lt;/strong&gt;(negative definite)；所有特征值都是非正数的矩阵称为&lt;strong&gt;半正定&lt;/strong&gt;(negative semidefinite)。半正定矩阵受到关注是因为它们保证$\forall x,x^{\top}Ax\ge0$。此外，正定矩阵还保证$x^{\top}Ax=0\Rightarrow x=0$。&lt;/p&gt;

&lt;h3 id=&quot;奇异值分解&quot;&gt;奇异值分解&lt;/h3&gt;

&lt;p&gt;将矩阵分解为&lt;strong&gt;奇异向量&lt;/strong&gt;(singular vector)和&lt;strong&gt;奇异值&lt;/strong&gt;(singular value)。通过奇异值分解，我们会得到一些特征分解相同类型的信息。每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。&lt;/p&gt;

&lt;p&gt;将矩阵$A$分解成三个矩阵的乘积:
&lt;script type=&quot;math/tex&quot;&gt;A=UDV^{\top}&lt;/script&gt;
假设$A$是一个$m\times n$的矩阵，那么$U$是一个$m\times m$的矩阵，$D$是一个$m\times n$的矩阵，$V$是一个$n\times n$的矩阵。&lt;/p&gt;

&lt;p&gt;矩阵$U$和$V$都定义为正交矩阵，而矩阵$D$定义为对角矩阵。矩阵$D$不一定是方阵。&lt;/p&gt;

&lt;p&gt;对角矩阵$D$对角线上的元素称为矩阵$A$的&lt;strong&gt;奇异值&lt;/strong&gt;(singular value)。矩阵$U$列向量称为&lt;strong&gt;左奇异向量&lt;/strong&gt;(left singular vector)，矩阵$V$的列向量称&lt;strong&gt;右奇异值&lt;/strong&gt;(right singular vector)。&lt;/p&gt;

&lt;p&gt;我们可以用与&lt;strong&gt;A&lt;/strong&gt;相关的特征分解去解释&lt;strong&gt;A&lt;/strong&gt;的奇异值分解。$A$的&lt;strong&gt;左奇异向量&lt;/strong&gt;(left singular vector)是$AA^{\top}$的特征向量。$A$的&lt;strong&gt;右奇异向量&lt;/strong&gt;(right singular vector)是$A^{\top}A$的特征向量。$A$的非零奇异值是$A^{\top}A$特征值的平方根，同时也是$AA^{\top}$特征值的平方根。&lt;/p&gt;

&lt;h3 id=&quot;moore-penrose-伪逆&quot;&gt;Moore-Penrose 伪逆&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^{+}=\lim_{a\to 0}(A^{\top}A+\alpha I)^{-1}A^{-1}&lt;/script&gt;

&lt;p&gt;计算伪逆的实际算法没有基于这个定义，而是使用下面的公式
&lt;script type=&quot;math/tex&quot;&gt;A^{+}=VD^{+}U^{\top}&lt;/script&gt;
对角矩阵$D$的伪逆$D^{+}$是其非零元素取倒数之后再转置得到的。&lt;/p&gt;

&lt;p&gt;当矩阵$A$的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x=A^{+}y$是方程所有可行解中欧几里得范数$\Arrowvert x \Arrowvert_{2}$最小的一个。&lt;/p&gt;

&lt;p&gt;当矩阵$A$的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离$\Arrowvert Ax-y \Arrowvert_{2}$最小。&lt;/p&gt;

&lt;h3 id=&quot;迹运算&quot;&gt;迹运算&lt;/h3&gt;

&lt;p&gt;迹运算返回的是矩阵对角元素的和：&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;Tr(A)=\sum_{i}A_{i,i}.&lt;/script&gt;
Frobenius范数：
&lt;script type=&quot;math/tex&quot;&gt;\Arrowvert A \Arrowvert_{F}=\sqrt{Tr(AA^{\top})}&lt;/script&gt;
性质：
&lt;script type=&quot;math/tex&quot;&gt;Tr(AB)=Tr(BA)&lt;/script&gt;
标量在迹运算后仍然是它自己：$a=Tr(a)$。&lt;/p&gt;

&lt;h2 id=&quot;概率与信息论&quot;&gt;概率与信息论&lt;/h2&gt;

&lt;h3 id=&quot;高斯分布&quot;&gt;高斯分布&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x;\mu,\sigma^{2})=\sqrt{\frac{1}{2\pi \sigma^{2}}}exp(-\frac{1}{2\sigma^{2}}(x-\mu)^{2})&lt;/script&gt;

&lt;h3 id=&quot;常用函数的有用性质&quot;&gt;常用函数的有用性质&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;$logistic$ $sigmoid$ 函数：&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma(x)=\frac{1}{1+exp(-x)}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$softplus$函数：&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\zeta(x)=log(1+exp(x))&lt;/script&gt;

&lt;h3 id=&quot;信息论&quot;&gt;信息论&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;KL散度&lt;/strong&gt;对于同一随机变量x有两个单独的概率分布$P(x)$和$Q(x)$，可以使用&lt;strong&gt;KL散度&lt;/strong&gt;(Kullback-Leibler divergence)来衡量这两个分布的差异：
&lt;script type=&quot;math/tex&quot;&gt;D_{KL}(P||Q)=\mathbb{E}_{x\sim P}[log\frac{P(x)}{Q(x)}]=\mathbb{E}_{x\sim P}[logP(x)-logQ(x)]&lt;/script&gt;
&lt;strong&gt;交叉熵&lt;/strong&gt;(cross-entropy)：
&lt;script type=&quot;math/tex&quot;&gt;H(P,Q)=H(P)+D_{KL}(P||Q)=-\mathbb{E}_{x\sim P}logQ(x)&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;数值计算&quot;&gt;数值计算&lt;/h2&gt;

&lt;h3 id=&quot;病态条件&quot;&gt;病态条件&lt;/h3&gt;

&lt;p&gt;条件数指的是函数相对于输入的微小变化而变化的快慢程度。&lt;/p&gt;

&lt;p&gt;考虑函数$f(x)=A^{-1}x$。当$A\in \mathbb{R}^{n\times n}$具有特征值分解时，其条件数为
&lt;script type=&quot;math/tex&quot;&gt;\max_{i,j}|\frac{\lambda_{i}}{\lambda_{j}}|&lt;/script&gt;
当该数很大时，矩阵求逆对输入的误差特别敏感。&lt;/p&gt;

&lt;h3 id=&quot;基于梯度的优化方法&quot;&gt;基于梯度的优化方法&lt;/h3&gt;

&lt;p&gt;我们把要最小化或最大化的函数称为&lt;strong&gt;目标函数&lt;/strong&gt;(objective function)或&lt;strong&gt;准则&lt;/strong&gt;(criterion)。当我们对其进行最小化时，也把它称为&lt;strong&gt;代价函数&lt;/strong&gt;(cost function)、&lt;strong&gt;损失函数&lt;/strong&gt;(loss function)或&lt;strong&gt;误差函数&lt;/strong&gt;(error function)。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;最速下降法&lt;/strong&gt;(method of steepest descent)或&lt;strong&gt;梯度下降&lt;/strong&gt;(gradient descent)。最速下降建议新的点为
&lt;script type=&quot;math/tex&quot;&gt;x^{\prime}=x-\epsilon \nabla_{x}f(x)&lt;/script&gt;
其中$\epsilon$为&lt;strong&gt;学习率&lt;/strong&gt;(learning rate)，是一个确定步长大小的正标量。&lt;/p&gt;

&lt;p&gt;$Hessian$矩阵：
&lt;script type=&quot;math/tex&quot;&gt;H(f)(x)_{i,j}=\frac{\partial ^{2}}{\partial x_{i} \partial x_{j}}f(x)&lt;/script&gt;
&lt;strong&gt;最优步长&lt;/strong&gt;：
&lt;script type=&quot;math/tex&quot;&gt;\epsilon^{*}=\frac{g^{\top}g}{g^{\top}Hg}&lt;/script&gt;
最坏的情况下，$g$与$H$最大特征值$\lambda _{max}$对应的特征向量对齐，则最优步长是$\frac{1}{\lambda _{max}}$。当我们要最小化的函数能用二次函数很好地近似的情况下，$Hessian$的特征值决定了学习率的量级。&lt;/p&gt;

&lt;p&gt;在深度学习的背景下，限制函数满足$Lipschitz$&lt;strong&gt;连续&lt;/strong&gt;(Lipschitz continuous)或其导数$Lipschitz$连续可以获得一些保证。$Lipschitz$连续函数的变化速度以$Lipschitz$&lt;strong&gt;常数&lt;/strong&gt;(Lipschitz constant) $\mathcal{L}$为界：
&lt;script type=&quot;math/tex&quot;&gt;\forall x,\forall y,|f(x)-f(y)|\le \mathcal{L}||x-y||_{2}&lt;/script&gt;&lt;/p&gt;</content><author><name>Metapunk</name></author><category term="notes" /><summary type="html">线性代数</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/motoko-1.jpg" /></entry></feed>