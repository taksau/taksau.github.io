<!doctype html>
<html>

<head>

  <title>
    
      深度学习预备知识(Part 1) | Metapunk
    
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <!-- Use Atom -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Metapunk" />
  <!-- RSS-v2.0
  <link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title="Metapunk | a place where I write down my notes and thoughts."/>
  //-->


  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto|Source+Code+Pro">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css">

  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'takchatsau', 'auto');
  ga('send', 'pageview');
</script>


  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.4.0 -->
<title>深度学习预备知识(Part 1) | Metapunk</title>
<meta name="generator" content="Jekyll v3.6.2" />
<meta property="og:title" content="深度学习预备知识(Part 1)" />
<meta name="author" content="Metapunk" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="线性代数" />
<meta property="og:description" content="线性代数" />
<link rel="canonical" href="http://localhost:4000/notes/Deeplearning.html" />
<meta property="og:url" content="http://localhost:4000/notes/Deeplearning.html" />
<meta property="og:site_name" content="Metapunk" />
<meta property="og:image" content="http://localhost:4000/motoko-1.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-04-29T00:00:00+08:00" />
<script type="application/ld+json">
{"datePublished":"2018-04-29T00:00:00+08:00","image":"http://localhost:4000/motoko-1.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/notes/Deeplearning.html"},"url":"http://localhost:4000/notes/Deeplearning.html","author":{"@type":"Person","name":"Metapunk"},"description":"线性代数","@type":"BlogPosting","headline":"深度学习预备知识(Part 1)","dateModified":"2018-04-29T00:00:00+08:00","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


</head>


<body>

<div class="container">
  <header class="site-header">
  <h3 class="site-title">
    <a href="/">Metapunk</a>
  </h3>
  <nav class="menu-list">
    
      <a href="/pages/notes.html" class="menu-link">Notes of my work</a>
    
      <a href="/pages/thoughts.html" class="menu-link">Thinking of my life</a>
    
      <a href="/pages/about.html" class="menu-link">About</a>
    

    
      <a href="https://twitter.com/takchatsau" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
    
      <a href="https://www.facebook.com/chatsau.tak" class="menu-link" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
    
      <a href="https://instagram.com/takchatsau" class="menu-link" target="_blank"><i class="fa fa-instagram" aria-hidden="true"></i></a>
    
      <a href="mailto:takchatsau@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
    
      <a href="feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
    
  </nav>
  <div class="dropdown">
    <button class="dropbtn"><i class="fa fa-bars" aria-hidden="true"></i></button>
    <div class="dropdown-content">
      
        <a href="/pages/notes.html" class="menu-link">Notes of my work</a>
      
        <a href="/pages/thoughts.html" class="menu-link">Thinking of my life</a>
      
        <a href="/pages/about.html" class="menu-link">About</a>
      

      
        <a href="https://twitter.com/takchatsau" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
      
        <a href="https://www.facebook.com/chatsau.tak" class="menu-link" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
      
        <a href="https://instagram.com/takchatsau" class="menu-link" target="_blank"><i class="fa fa-instagram" aria-hidden="true"></i></a>
      
        <a href="mailto:takchatsau@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
      
        <a href="feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
      
    </div>
  </div>
</header>

  <div class="posts-wrapper">
    <div class="page-content">
  <h1>
    深度学习预备知识(Part 1)
  </h1>

  <span class="post-date">
    Written on
    
    April
    29th,
    2018
    by
    
      Metapunk
    
  </span>

  
    <div class="featured-image">
      <img src="/assets/img/motoko-1.jpg">
    </div>
  

  <article>
    <h2 id="线性代数">线性代数</h2>

<h4 id="向量的范数norm">向量的<strong>范数</strong>(norm)</h4>

<script type="math/tex; mode=display">||x||_{p}=(\sum_{i}|x|^{p})^\frac{1}{p} \\
其中p\in \mathbb{R},p\ge 1</script>

<p>$L^{1}$范数经常作为表示非零元素数目的替代函数</p>

<p><strong>最大范数$L^{\infty}$：</strong>表示向量中具有最大幅值的元素的绝对值
<script type="math/tex">||x||_{\infty}=\max_{i}|x_{i}|</script>
 在深度学习中，会衡量矩阵的大小，最常用<strong>Frobenius范数</strong> (Frobenius norm)
<script type="math/tex">||A||_{F}=\sqrt{\sum_{i,j}A^{2}_{i,j}}</script></p>

<h3 id="特殊类型的矩阵和向量">特殊类型的矩阵和向量</h3>

<ul>
  <li>
    <p><strong>对角矩阵</strong>(diagonal matrix)只在主对角线上含有非零元素，其他位置都是零。用diag(v)表示。</p>
  </li>
  <li>
    <p><strong>对称矩阵</strong>(symmetric matrix)是转置和自己相等的矩阵。</p>
  </li>
  <li>
    <p><strong>正交矩阵</strong>(orthogonal matrix)指行向量和列向量是分别标准正交的方阵，即
<script type="math/tex">A^{\top}A=AA^{\top}=I \\
 A^{-1}=A^{\top}</script></p>
  </li>
</ul>

<h3 id="特征分解">特征分解</h3>

<p>方阵$A$的<strong>特征向量</strong> (eigenvector)是指与$A$相乘后相当于对该向量进行缩放的非零向量<strong>v</strong>：
<script type="math/tex">\mathit{Av=\lambda v}</script>
其中标量$\mathit{\lambda}$称为这个特征向量对应的<strong>特征值</strong>(eigenvalue)。</p>

<p>假设矩阵$A$ 有$n$ 个线性无关的特征向量${v^{(1)},\cdots,v^{(n)}}$，对应着特征值${\lambda_{1},\cdots,\lambda_{n}}$。 将特征向量连接成一个矩阵，使得每一列是一个特征向量：$V=[v^{(1)},\cdots,v^{(n)}]$ 。类似地，将特征值连接成一个向量$\lambda=[\lambda_{1},\cdots,\lambda_{n}]^{\top}$。因此$A$的<strong>特征分解</strong>(eigendecomposition)可以记作：
<script type="math/tex">A=Vdiag(\lambda)V^{-1}</script></p>

<p>每个实对称矩阵都可以分解成实特征向量和实特征值：</p>

<script type="math/tex; mode=display">A=Q\Lambda Q^{\top}</script>

<p>其中$Q$是$A$的特征向量组成的正交矩阵，$\Lambda$是对角矩阵。特征值$\Lambda_{i,i}$对应的特征向量是矩阵$Q$的第$i$列，记作$Q_{:,i}$。</p>

<p>如果两个或多个特征向量拥有相同的特征值，那么在由这些特征向量产生的生成子空间中，任意一组正交向量都是该特征值对应的特征向量。因此，我们可以等价地从这些特征向量中构成$Q$作为替代。按照惯例，我们通常按降序排列$\Lambda$的元素。在该约定下，特征分解唯一，当且仅当所有的特征值都是唯一的。</p>

<p>所有特征值都是正数的矩阵称为<strong>正定</strong>(positive definite)；所有特征值都是非负数的矩阵称为<strong>半正定</strong>(positive semidefinite)。同样地，所有特征值都是负数的<strong>负定</strong>(negative definite)；所有特征值都是非正数的矩阵称为<strong>半正定</strong>(negative semidefinite)。半正定矩阵受到关注是因为它们保证$\forall x,x^{\top}Ax\ge0$。此外，正定矩阵还保证$x^{\top}Ax=0\Rightarrow x=0$。</p>

<h3 id="奇异值分解">奇异值分解</h3>

<p>将矩阵分解为<strong>奇异向量</strong>(singular vector)和<strong>奇异值</strong>(singular value)。通过奇异值分解，我们会得到一些特征分解相同类型的信息。每个实数矩阵都有一个奇异值分解，但不一定都有特征分解。</p>

<p>将矩阵$A$分解成三个矩阵的乘积:
<script type="math/tex">A=UDV^{\top}</script>
假设$A$是一个$m\times n$的矩阵，那么$U$是一个$m\times m$的矩阵，$D$是一个$m\times n$的矩阵，$V$是一个$n\times n$的矩阵。</p>

<p>矩阵$U$和$V$都定义为正交矩阵，而矩阵$D$定义为对角矩阵。矩阵$D$不一定是方阵。</p>

<p>对角矩阵$D$对角线上的元素称为矩阵$A$的<strong>奇异值</strong>(singular value)。矩阵$U$列向量称为<strong>左奇异向量</strong>(left singular vector)，矩阵$V$的列向量称<strong>右奇异值</strong>(right singular vector)。</p>

<p>我们可以用与<strong>A</strong>相关的特征分解去解释<strong>A</strong>的奇异值分解。$A$的<strong>左奇异向量</strong>(left singular vector)是$AA^{\top}$的特征向量。$A$的<strong>右奇异向量</strong>(right singular vector)是$A^{\top}A$的特征向量。$A$的非零奇异值是$A^{\top}A$特征值的平方根，同时也是$AA^{\top}$特征值的平方根。</p>

<h3 id="moore-penrose-伪逆">Moore-Penrose 伪逆</h3>

<script type="math/tex; mode=display">A^{+}=\lim_{a\to 0}(A^{\top}A+\alpha I)^{-1}A^{-1}</script>

<p>计算伪逆的实际算法没有基于这个定义，而是使用下面的公式
<script type="math/tex">A^{+}=VD^{+}U^{\top}</script>
对角矩阵$D$的伪逆$D^{+}$是其非零元素取倒数之后再转置得到的。</p>

<p>当矩阵$A$的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一种。特别地，$x=A^{+}y$是方程所有可行解中欧几里得范数$\Arrowvert x \Arrowvert_{2}$最小的一个。</p>

<p>当矩阵$A$的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离$\Arrowvert Ax-y \Arrowvert_{2}$最小。</p>

<h3 id="迹运算">迹运算</h3>

<p>迹运算返回的是矩阵对角元素的和：</p>

<p><script type="math/tex">Tr(A)=\sum_{i}A_{i,i}.</script>
Frobenius范数：
<script type="math/tex">\Arrowvert A \Arrowvert_{F}=\sqrt{Tr(AA^{\top})}</script>
性质：
<script type="math/tex">Tr(AB)=Tr(BA)</script>
标量在迹运算后仍然是它自己：$a=Tr(a)$。</p>

<h2 id="概率与信息论">概率与信息论</h2>

<h3 id="高斯分布">高斯分布</h3>

<script type="math/tex; mode=display">\mathcal{N}(x;\mu,\sigma^{2})=\sqrt{\frac{1}{2\pi \sigma^{2}}}exp(-\frac{1}{2\sigma^{2}}(x-\mu)^{2})</script>

<h3 id="常用函数的有用性质">常用函数的有用性质</h3>

<ul>
  <li>$logistic$ $sigmoid$ 函数：</li>
</ul>

<script type="math/tex; mode=display">\sigma(x)=\frac{1}{1+exp(-x)}</script>

<ul>
  <li>$softplus$函数：</li>
</ul>

<script type="math/tex; mode=display">\zeta(x)=log(1+exp(x))</script>

<h3 id="信息论">信息论</h3>

<p><strong>KL散度</strong>对于同一随机变量x有两个单独的概率分布$P(x)$和$Q(x)$，可以使用<strong>KL散度</strong>(Kullback-Leibler divergence)来衡量这两个分布的差异：
<script type="math/tex">D_{KL}(P||Q)=\mathbb{E}_{x\sim P}[log\frac{P(x)}{Q(x)}]=\mathbb{E}_{x\sim P}[logP(x)-logQ(x)]</script>
<strong>交叉熵</strong>(cross-entropy)：
<script type="math/tex">H(P,Q)=H(P)+D_{KL}(P||Q)=-\mathbb{E}_{x\sim P}logQ(x)</script></p>

<h2 id="数值计算">数值计算</h2>

<h3 id="病态条件">病态条件</h3>

<p>条件数指的是函数相对于输入的微小变化而变化的快慢程度。</p>

<p>考虑函数$f(x)=A^{-1}x$。当$A\in \mathbb{R}^{n\times n}$具有特征值分解时，其条件数为
<script type="math/tex">\max_{i,j}|\frac{\lambda_{i}}{\lambda_{j}}|</script>
当该数很大时，矩阵求逆对输入的误差特别敏感。</p>

<h3 id="基于梯度的优化方法">基于梯度的优化方法</h3>

<p>我们把要最小化或最大化的函数称为<strong>目标函数</strong>(objective function)或<strong>准则</strong>(criterion)。当我们对其进行最小化时，也把它称为<strong>代价函数</strong>(cost function)、<strong>损失函数</strong>(loss function)或<strong>误差函数</strong>(error function)。</p>

<p><strong>最速下降法</strong>(method of steepest descent)或<strong>梯度下降</strong>(gradient descent)。最速下降建议新的点为
<script type="math/tex">x^{\prime}=x-\epsilon \nabla_{x}f(x)</script>
其中$\epsilon$为<strong>学习率</strong>(learning rate)，是一个确定步长大小的正标量。</p>

<p>$Hessian$矩阵：
<script type="math/tex">H(f)(x)_{i,j}=\frac{\partial ^{2}}{\partial x_{i} \partial x_{j}}f(x)</script>
<strong>最优步长</strong>：
<script type="math/tex">\epsilon^{*}=\frac{g^{\top}g}{g^{\top}Hg}</script>
最坏的情况下，$g$与$H$最大特征值$\lambda _{max}$对应的特征向量对齐，则最优步长是$\frac{1}{\lambda _{max}}$。当我们要最小化的函数能用二次函数很好地近似的情况下，$Hessian$的特征值决定了学习率的量级。</p>

<p>在深度学习的背景下，限制函数满足$Lipschitz$<strong>连续</strong>(Lipschitz continuous)或其导数$Lipschitz$连续可以获得一些保证。$Lipschitz$连续函数的变化速度以$Lipschitz$<strong>常数</strong>(Lipschitz constant) $\mathcal{L}$为界：
<script type="math/tex">\forall x,\forall y,|f(x)-f(y)|\le \mathcal{L}||x-y||_{2}</script></p>

<h3 id="约束优化">约束优化</h3>

<p><strong>Karush-Kuhn-Tucker</strong>(KKT)方法是针对约束优化非常通用的解决方案。为介绍KKT方法，我们引入一个称为<strong>广义Lagrangian</strong>(generalized Lagrangian)。
<script type="math/tex">\mathbb{S}=\{ x|\forall i,g^{(i)}(x)=0\ and \ \forall j,h^{(j)}(x)\le0\}</script>
我们为每个约束引入新的变量$\lambda_{i}$ 和$\alpha_{j}$，这些新变量被称为KKT乘子。
<script type="math/tex">L(x,\lambda,\alpha)=f(x)+\sum_{i}\lambda_{i}g^{(i)}(x)+\sum_{j}\alpha_{j}h^{(j)}(x)</script></p>

  </article>

  <div class="post-share">
    <div class="post-date">Feel free to share!</div>
    <div class="sharing-icons">
      <a href="https://twitter.com/intent/tweet?text=深度学习预备知识(Part 1)&amp;url=/notes/Deeplearning.html" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
      <a href="https://www.facebook.com/sharer/sharer.php?u=/notes/Deeplearning.html&amp;title=深度学习预备知识(Part 1)" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
      <a href="https://plus.google.com/share?url=/notes/Deeplearning.html" target="_blank"><i class="fa fa-google-plus" aria-hidden="true"></i></a>
    </div>
  </div>

  <div class="related">
    <h2>You may also enjoy...</h2>
    
    <ul class="related-posts">
      
        
          
          
        
      
    </ul>
  </div>

  

</div>

  </div>
  <footer class="footer">
  
    <a href="https://twitter.com/takchatsau" class="menu-link" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  
    <a href="https://www.facebook.com/chatsau.tak" class="menu-link" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
  
    <a href="https://instagram.com/takchatsau" class="menu-link" target="_blank"><i class="fa fa-instagram" aria-hidden="true"></i></a>
  
    <a href="mailto:takchatsau@gmail.com" class="menu-link" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  
    <a href="feed.xml" class="menu-link" target="_blank"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  
  <div class="post-date"><a href="/">Metapunk | a place where I write down my notes and thoughts. by Takchatsau</a></div>
</footer>

</div>

</body>
</html>
