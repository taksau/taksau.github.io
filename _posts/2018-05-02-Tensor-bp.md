---
layout: post
title: "向量、矩阵运算的求导"
author: "Metapunk"
categories: notes
tags: [notes]
image: neon.jpg
---

## Jacobian矩阵

对于映射$f:\mathbb{R}^{N}\rightarrow\mathbb{R}^{M}$，对各个output求导会获得一个$M\times N$的偏微分矩阵：
$$
\frac{\partial y}{\partial x}=\left(\begin{matrix}
\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{N}} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_{M}}{\partial x_{1}} & \cdots & \frac{\partial y_{M}}{\partial x_{N}}
\end{matrix}\right)
$$
于是显然可得，在线性分类器中，$\vec{y}=W\vec{x}$，x对y的偏导有简单结果：
$$
\frac{\partial \vec{y}_{i}}{\partial \vec{x}_{j}}=W_{i,j}
\\
即
\\
\frac{\partial \vec{y}}{\partial \vec{x}}=W
$$

**对矩阵相乘:**
$$
Y=XW \\
 Y_{i,j}=\sum^{D}_{k=1}X_{i,k}{W_{k,j}} \\
 \frac{\partial Y_{i,j}}{\partial X_{i,k} }=W_{k,j}
 \\
 则，\frac{\partial Y_{i,:}}{\partial X_{i,:} }=W，其余情况均为0。
$$

### 链式法则：

上述形式也能适用链式法则，对：

$\vec{y}=VW\vec{x}$，显然，$\frac{d\vec{y}}{d\vec{x}}=VW$。

定义中间变量：$\vec{m}=W\vec{x}$，则$\vec{y}=V\vec{m}$。

依照链式法则：
$$
\frac{d\vec{y}}{d\vec{x}}=\frac{d\vec{y}}{d\vec{m}}\frac{d\vec{m}}{d\vec{x}}
\\
\frac{d\vec{y_{i}}}{d\vec{x_{j}}}=\frac{d\vec{y_{i}}}{d\vec{m}}\frac{d\vec{m}}{d\vec{x_{j}}}=\sum^{M}_{k=1}\frac{d\vec{y_{i}}}{d\vec{m_{k}}}\frac{d\vec{m_{k}}}{d\vec{x_{j}}}
\\
\frac{d\vec{y_{i}}}{d\vec{x_{j}}}=(VK)_{i,j}=\sum^{M}_{k=1}V_{i,k}W_{k,j}
$$
即对于矩阵或向量的复合函数求导，亦可以依照链式法则，将各中间变量的雅可比矩阵相乘得到最后的偏导。

